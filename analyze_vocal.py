"""
ë³´ì»¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì •êµí•œ ìŒì • íƒ€ì„ë¼ì¸ì„ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
librosaì˜ PYIN ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ìŒì • ê°ì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import json
import numpy as np
import os
import sys

try:
    import librosa
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    print('ì˜¤ë¥˜: librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')

def get_vocal_start_time(audio_data, sample_rate, top_db=20):
    """
    ë³´ì»¬ ì˜¤ë””ì˜¤ì—ì„œ ì‹¤ì œë¡œ ì†Œë¦¬ê°€ ë‚˜ê¸° ì‹œì‘í•˜ëŠ” ì‹œì ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        audio_data: ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        top_db: ë¬´ìŒ êµ¬ê°„ íŒì • ì„ê³„ê°’ (dB)
    
    Returns:
        ì²« ë³´ì»¬ ì‹œì‘ ì‹œê°„ (ì´ˆ)
    """
    try:
        # ë¬´ìŒ êµ¬ê°„ ì œê±° (Non-silent êµ¬ê°„ íƒì§€)
        intervals = librosa.effects.split(audio_data, top_db=top_db)
        
        if len(intervals) == 0:
            print("  âš ï¸ ë³´ì»¬ ì†Œë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (0ì´ˆì—ì„œ ì‹œì‘)")
            return 0.0
        
        # ì²« ë²ˆì§¸ êµ¬ê°„ì˜ ì‹œì‘ ìƒ˜í”Œ ì°¾ê¸°
        first_vocal_sample = intervals[0][0]
        
        # ìƒ˜í”Œ -> ì‹œê°„(ì´ˆ) ë³€í™˜
        start_time = librosa.samples_to_time(first_vocal_sample, sr=sample_rate)
        print(f"  ğŸ¤ ì‹¤ì œ ë³´ì»¬ ì‹œì‘ ì‹œê°„: {start_time:.2f}ì´ˆ")
        
        return start_time
    except Exception as e:
        print(f"  âš ï¸ ë³´ì»¬ ì‹œì‘ ì‹œì  íƒì§€ ì˜¤ë¥˜: {e} (0ì´ˆë¡œ ì„¤ì •)")
        return 0.0

def analyze_vocal_with_pyin(audio_file_path, time_resolution=0.05):
    """
    PYIN ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë³´ì»¬ ì˜¤ë””ì˜¤ì˜ ìŒì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        audio_file_path: ë³´ì»¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        time_resolution: ì‹œê°„ í•´ìƒë„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’ 0.05ì´ˆ)
    
    Returns:
        íƒ€ì„ë¼ì¸ë³„ ìŒì • ë°ì´í„° ë¦¬ìŠ¤íŠ¸, ì²« ë³´ì»¬ ì‹œì‘ ì‹œê°„
    """
    if not HAS_LIBROSA:
        return None, None
    
    try:
        print(f"ë³´ì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {audio_file_path}")
        # ì˜¤ë””ì˜¤ ë¡œë“œ (ëª¨ë…¸, 44100Hz)
        audio_data, sample_rate = librosa.load(audio_file_path, sr=44100, mono=True)
        duration = len(audio_data) / sample_rate
        print(f"  ê¸¸ì´: {duration:.2f}ì´ˆ, ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sample_rate}Hz")
        
        # ì‹¤ì œ ë³´ì»¬ ì‹œì‘ ì‹œì  ì°¾ê¸°
        print("ë³´ì»¬ ì‹œì‘ ì‹œì  íƒì§€ ì¤‘...")
        vocal_start_time = get_vocal_start_time(audio_data, sample_rate, top_db=20)
        
        # PYIN ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìŒì • ì¶”ì¶œ (ë” ì •í™•í•¨)
        print("PYIN ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìŒì • ë¶„ì„ ì¤‘...")
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio_data,
            fmin=librosa.note_to_hz('C2'),  # ìµœì†Œ ì£¼íŒŒìˆ˜ (ì•½ 65Hz)
            fmax=librosa.note_to_hz('C7'),   # ìµœëŒ€ ì£¼íŒŒìˆ˜ (ì•½ 2093Hz)
            frame_length=2048,
            hop_length=512
        )
        
        # ë³¼ë¥¨(ì‹ í˜¸ ê°•ë„) ê³„ì‚° - ì‹¤ì œ ë³´ì»¬ êµ¬ê°„ íŒë³„ìš©
        print("ë³¼ë¥¨ ë¶„ì„ ì¤‘... (ì‹¤ì œ ë³´ì»¬ êµ¬ê°„ í•„í„°ë§)")
        rms = librosa.feature.rms(y=audio_data, frame_length=2048, hop_length=512)[0]
        rms_times = librosa.frames_to_time(np.arange(len(rms)), sr=sample_rate, hop_length=512)
        
        # ë³¼ë¥¨ ì„ê³„ê°’ ê³„ì‚° (ì „ì²´ RMSì˜ ìƒìœ„ 10% ê¸°ì¤€ - ë” ê´€ëŒ€í•˜ê²Œ)
        rms_sorted = np.sort(rms)
        volume_threshold = np.percentile(rms_sorted, 10)  # í•˜ìœ„ 10%ë§Œ ì œì™¸ (ë” ê´€ëŒ€)
        
        # ë¬´ìŒ êµ¬ê°„ ì°¾ê¸° (librosa.effects.split ì‚¬ìš©)
        intervals = librosa.effects.split(audio_data, top_db=20)
        silent_regions = []
        if len(intervals) > 0:
            # ë¬´ìŒ êµ¬ê°„ ì°¾ê¸° (intervals ì‚¬ì´ì˜ ê³µê°„)
            for i in range(len(intervals) - 1):
                silent_start = librosa.samples_to_time(intervals[i][1], sr=sample_rate)
                silent_end = librosa.samples_to_time(intervals[i + 1][0], sr=sample_rate)
                silent_regions.append((silent_start, silent_end))
        
        # ì‹œê°„ì¶• ìƒì„±
        times = librosa.frames_to_time(np.arange(len(f0)), sr=sample_rate, hop_length=512)
        
        # íƒ€ì„ë¼ì¸ë³„ ë°ì´í„° ìƒì„±
        timeline_data = []
        current_time = 0
        
        print(f"íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„± ì¤‘... (í•´ìƒë„: {time_resolution}ì´ˆ)")
        
        while current_time <= duration:
            # ì‹¤ì œ ë³´ì»¬ ì‹œì‘ ì‹œì  ì´ì „ì€ ëª¨ë‘ ë°˜ì£¼ êµ¬ê°„ìœ¼ë¡œ ì²˜ë¦¬
            if current_time < vocal_start_time:
                timeline_data.append({
                    'time': round(current_time, 3),
                    'pitch': None
                })
            else:
                # ë¬´ìŒ êµ¬ê°„ ì²´í¬
                is_silent = False
                for silent_start, silent_end in silent_regions:
                    if silent_start <= current_time <= silent_end:
                        is_silent = True
                        break
                
                if is_silent:
                    timeline_data.append({
                        'time': round(current_time, 3),
                        'pitch': None
                    })
                else:
                    # ê°€ì¥ ê°€ê¹Œìš´ í”„ë ˆì„ ì°¾ê¸°
                    frame_idx = np.argmin(np.abs(times - current_time))
                    rms_idx = np.argmin(np.abs(rms_times - current_time))
                    
                    if frame_idx < len(f0) and rms_idx < len(rms):
                        pitch = f0[frame_idx]
                        is_voiced = voiced_flag[frame_idx] if frame_idx < len(voiced_flag) else False
                        volume = rms[rms_idx] if rms_idx < len(rms) else 0
                        confidence = voiced_probs[frame_idx] if frame_idx < len(voiced_probs) else 0
                        
                        # ì‹¤ì œ ë³´ì»¬ íŒë³„ ì¡°ê±´:
                        # 1. voicedê°€ True
                        # 2. pitchê°€ ìœ íš¨
                        # 3. ë³¼ë¥¨ì´ ì„ê³„ê°’ ì´ìƒ (ì‹¤ì œ ë³´ì»¬ ì†Œë¦¬) - ë˜ëŠ” ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ í—ˆìš©
                        # 4. ì‹ ë¢°ë„ê°€ 0.3 ì´ìƒ (ë” ê´€ëŒ€í•˜ê²Œ)
                        if (is_voiced and not np.isnan(pitch) and pitch > 0 and 
                            (volume >= volume_threshold or confidence >= 0.7) and confidence >= 0.2):
                            timeline_data.append({
                                'time': round(current_time, 3),
                                'pitch': round(float(pitch), 2)
                            })
                        else:
                            timeline_data.append({
                                'time': round(current_time, 3),
                                'pitch': None
                            })
                    else:
                        timeline_data.append({
                            'time': round(current_time, 3),
                            'pitch': None
                        })
            
            current_time += time_resolution
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            progress = (current_time / duration) * 100
            if int(progress) % 10 == 0 and len(timeline_data) % 20 == 0:
                print(f"  ì§„í–‰ë¥ : {progress:.0f}%")
        
        return timeline_data, vocal_start_time
    
    except Exception as e:
        print(f'ìŒì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}')
        import traceback
        traceback.print_exc()
        return None, None

def extract_rhythm_info(audio_file_path):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ë¦¬ë“¬ ì •ë³´(BPM, ë°•ì)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        BPM, ë°•ì ì‹œì‘ ì‹œì  ë¦¬ìŠ¤íŠ¸
    """
    if not HAS_LIBROSA:
        return None, None
    
    try:
        print("ë¦¬ë“¬ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        audio_data, sample_rate = librosa.load(audio_file_path, sr=44100, mono=True)
        
        # BPM ì¶”ì¶œ
        tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
        beat_times = librosa.frames_to_time(beats, sr=sample_rate)
        
        # numpy ë°°ì—´ì„ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        tempo_value = float(tempo) if hasattr(tempo, 'item') else float(tempo)
        beat_times_list = [float(t) for t in beat_times] if hasattr(beat_times, 'tolist') else list(beat_times)
        
        print(f"  BPM: {tempo_value:.1f}")
        print(f"  ë°•ì ìˆ˜: {len(beats)}ê°œ")
        
        return tempo_value, beat_times_list
    
    except Exception as e:
        print(f'ë¦¬ë“¬ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}')
        return None, None

def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python analyze_vocal.py <ë³´ì»¬_ì˜¤ë””ì˜¤_íŒŒì¼> [ì¶œë ¥_JSON_íŒŒì¼]")
        sys.exit(1)
    
    vocal_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else 'static/assets/music_pitch_data.json'
    
    if not os.path.exists(vocal_file):
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vocal_file}")
        sys.exit(1)
    
    print(f"ë³´ì»¬ ìŒì • ë¶„ì„ ì‹œì‘: {vocal_file}")
    print(f"ì¶œë ¥ íŒŒì¼: {output_file}\n")
    
    # ìŒì • íƒ€ì„ë¼ì¸ ë¶„ì„
    timeline_data, vocal_start_time = analyze_vocal_with_pyin(vocal_file, time_resolution=0.05)
    
    if not timeline_data:
        print("ë¶„ì„ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ë¦¬ë“¬ ì •ë³´ ì¶”ì¶œ
    bpm, beat_times = extract_rhythm_info(vocal_file)
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'timeline': timeline_data,
        'metadata': {
            'bpm': bpm,
            'beat_times': beat_times,
            'source_file': vocal_file,
            'source_type': 'vocal_audio_only',  # music1.mp4ì˜ ë³´ì»¬ë§Œ ì‚¬ìš©
            'vocal_start_time': vocal_start_time  # ì‹¤ì œ ë³´ì»¬ ì‹œì‘ ì‹œì 
        }
    }
    
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"  ë°ì´í„° í¬ì¸íŠ¸: {len(timeline_data)}ê°œ")
    print(f"  ê²°ê³¼ ì €ì¥: {output_file}")
    
    # í†µê³„ ì •ë³´
    pitches = [d['pitch'] for d in timeline_data if d['pitch']]
    if pitches:
        print(f"\ní†µê³„:")
        print(f"  ìœ íš¨í•œ ìŒì • ë°ì´í„°: {len(pitches)}/{len(timeline_data)}")
        print(f"  í‰ê·  ìŒì •: {np.mean(pitches):.2f} Hz")
        print(f"  ìµœì†Œ ìŒì •: {np.min(pitches):.2f} Hz")
        print(f"  ìµœëŒ€ ìŒì •: {np.max(pitches):.2f} Hz")

if __name__ == '__main__':
    main()

